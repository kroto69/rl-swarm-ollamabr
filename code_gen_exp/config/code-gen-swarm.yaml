log_dir: ${oc.env:ROOT,.}/logs

hydra:
  run:
    dir: ${log_dir}
  job_logging:
    handlers:
      console:
        level: INFO
    root:
      level: INFO

training:
  max_round: 1000000
  max_stage: 1
  hf_push_frequency: 1
  # [OPTIMISASI] RAM 32GB kuat nampung 4 generasi.
  # Ini bikin training GRPO jauh lebih efektif.
  num_generations: 4 
  num_transplant_trees: 2
  seed: 42
  dtype: 'float32'
  # [OPTIMISASI] Panjang token cukup untuk fungsi python lengkap.
  max_new_tokens: 768 

reward_config:
  # Pastikan Ollama jalan di port/server yang tidak membebani training loop
  ollama_model: qwen2.5-coder-32b-instruct
  temperature: 0.0
  num_predict: 256

blockchain:
  alchemy_url: "https://gensyn-testnet.g.alchemy.com/public"
  swarm_contract_address: ${oc.env:SWARM_CONTRACT,null}
  org_id: ${oc.env:ORG_ID,null}
  mainnet_chain_id: 685685
  modal_proxy_url: "http://localhost:3000/api/"
  swarm_coordinator_abi_path: "code_gen_exp/contracts/SwarmCoordinator_0.4.2.json"

eval:
  judge_base_url: "https://codezero-judge.gensyn.ai"

game_manager:
  _target_: code_gen_exp.src.manager.SwarmGameManager
  max_stage: ${training.max_stage}
  max_round: ${training.max_round}
  log_dir: ${log_dir}
  hf_token: ${oc.env:HUGGINGFACE_ACCESS_TOKEN,null}
  hf_push_frequency: ${training.hf_push_frequency}
  rewards_ollama_model: ${reward_config.ollama_model}
  run_mode: "train_and_evaluate"
  
  # [FIX BUG] Baris ini yang sebelumnya hilang menyebabkan crash
  coordinator: ${coordinator}

  game_state: 
    _target_: genrl.state.game_state.GameState
    round: 0
    stage: 0
  trainer:
    _target_: code_gen_exp.src.trainer.GRPOTrainerModule
    models:
      - _target_: transformers.AutoModelForCausalLM.from_pretrained
        pretrained_model_name_or_path: ${oc.env:MODEL_NAME, ${gpu_model_choice:${default_large_model_pool},${default_small_model_pool}}} 
    config:
      _target_: genrl.trainer.grpo_trainer.GRPOTrainerConfig
      dtype: ${training.dtype}
      # Epsilon dinaikkan sedikit untuk stabilitas dengan num_generations 4
      epsilon: 0.2 
      epsilon_high: 0.28
      max_new_tokens: ${training.max_new_tokens}
      num_generations: ${training.num_generations}
    log_with: wandb
    log_dir: ${log_dir}
    judge_base_url: ${eval.judge_base_url}
  reward_manager:
    _target_: genrl.rewards.DefaultRewardManager
    reward_fn_store:
      _target_: genrl.rewards.reward_store.RewardFnStore
      max_rounds: ${training.max_round}
      reward_fn_stores:
        - _target_: genrl.rewards.reward_store.RoundRewardFnStore
          num_stages: ${training.max_stage}
          reward_fns:
            - _target_: code_gen_exp.src.solver_rewards.CodeGenerationRewards
              solver_tokenizer_path: ${game_manager.trainer.models.0.pretrained_model_name_or_path}
              solver_token_lim: ${training.max_new_tokens}
              ollama_config:
                _target_: code_gen_exp.src.solver_rewards.RewardsOllamaConfig
                model: ${reward_config.ollama_model}
                temperature: ${reward_config.temperature}
                num_predict: ${reward_config.num_predict}
  data_manager:
    _target_: code_gen_exp.src.solver_data.CodeGenerationDataManager
    system_prompt: 'solver'
    # [OPTIMISASI] Batch size 4 agar RAM 32GB terpakai optimal
    batch_size: 4 
    local_batch_size: 2
    proposer_batch_size: 2
    num_generations: ${training.num_generations}
    num_transplant_trees: ${training.num_transplant_trees}
  communication_kwargs:
    identity_path: ${oc.env:IDENTITY_PATH,null}
    # [SAFETY] Timeout diperpanjang ke 5 menit agar tidak crash di CPU
    startup_timeout: 400 
    beam_size: 5
    get_retries: 5

# Definisi Coordinator (Diambil oleh game_manager via ${coordinator})
coordinator:
  _target_: code_gen_exp.src.coordinator.ModalSwarmCoordinator
  web3_url: ${blockchain.alchemy_url}
  contract_address: ${blockchain.swarm_contract_address}
  org_id: ${blockchain.org_id}
  modal_proxy_url: ${blockchain.modal_proxy_url}
  swarm_coordinator_abi_json: ${blockchain.swarm_coordinator_abi_path}

default_large_model_pool: 
  - Qwen/Qwen2.5-Coder-1.5B-Instruct

default_small_model_pool:
  - Qwen/Qwen2.5-Coder-0.5B-Instruct

proposer:
  _target_: code_gen_exp.src.proposer_service.ProposerService
  service_config:
    _target_: code_gen_exp.src.proposer_service.ProposerServiceConfig
    model: ${oc.env:MODEL_NAME, ${gpu_model_choice:${default_large_model_pool},${default_small_model_pool}}} 
    num_proposals: 4
    train_batch_size: 4    
    identity_path: ${oc.env:IDENTITY_PATH,null}
    startup_timeout: 300
    beam_size: 5
    get_retries: 5
  ppo_config:
    _target_: code_gen_exp.src.proposer.PPOConfig
  vllm_config:
    _target_: code_gen_exp.src.proposer.VllmConfig
  coordinator: ${coordinator}
